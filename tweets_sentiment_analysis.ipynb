{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://www.youtube.com/watch?v=ujId4ipkBio\n",
    "# https://medium.com/better-programming/twitter-sentiment-analysis-15d8892c0082\n",
    "# https://www.youtube.com/watch?v=eFdPGpny_hY\n",
    "# https://www.youtube.com/watch?v=1gQ6uG5Ujiw&t=2854s\n",
    "# https://monkeylearn.com/blog/sentiment-analysis-of-twitter/\n",
    "\n",
    "from tweepy import API \n",
    "from tweepy import Cursor\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    " \n",
    "import twitter_credentials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "#Twitter Authenticator uses data from individual's twitter credentials \n",
    "class TwitterAuthenticator():\n",
    "    def authenticate_twitter_app(self):\n",
    "        auth = OAuthHandler(twitter_credentials.CONSUMER_KEY, twitter_credentials.CONSUMER_SECRET)\n",
    "        auth.set_access_token(twitter_credentials.ACCESS_TOKEN, twitter_credentials.ACCESS_TOKEN_SECRET)\n",
    "        return auth\n",
    "\n",
    "# TwitterClient\n",
    "class TwitterClient():\n",
    "    def __init__(self, twitter_user = None):\n",
    "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\n",
    "        self.twitter_client = API(self.auth)\n",
    "\n",
    "        self.twitter_user = twitter_user\n",
    "\n",
    "    def get_twitter_client_api(self):\n",
    "        return self.twitter_client\n",
    "    def get_home_timeline_tweets(self, num_tweets):\n",
    "        home_timeline_tweets = []\n",
    "        for tweet in Cursor(self.twitter_client.home_timeline, id=self.twitter_user).items(num_tweets):\n",
    "            home_timeline_tweets.append(tweet)\n",
    "        return home_timeline_tweets\n",
    "\n",
    "    def get_user_timeline_tweets(self, num_tweets):\n",
    "        tweets = []\n",
    "        for tweet in Cursor(self.twitter_client.user_timeline, id=self.twitter_user).items(num_tweets):\n",
    "            tweets.append(tweet)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "# prints out tweets\n",
    "class TwitterListener(StreamListener):\n",
    "\n",
    "    def __init__(self, fetched_tweets_filename):\n",
    "        self.fetched_tweets_filename = fetched_tweets_filename\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            print(data)\n",
    "            with open(self.fetched_tweets_filename, 'a') as tf:\n",
    "                tf.write(data)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data %s\" % str(e))\n",
    "        return True\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweets                   id  \\\n",
      "0   @JoshSchoen @AlexBerenson @neil_ferguson Utter...  1257757474997690368   \n",
      "1   @AlexBerenson @neil_ferguson Thanks! Something...  1257756154983153664   \n",
      "2            @AlexBerenson @neil_ferguson What a tool  1257755091894800385   \n",
      "3   @flcnhvy @TeslaGong @PPathole @priscillabanana...  1257559216258084864   \n",
      "4   @PicklePunchD @Jason @TeslaGong @PPathole @pri...  1257554434764103680   \n",
      "5   @Jason @TeslaGong @PPathole @priscillabanana T...  1257552183601422337   \n",
      "6   @TeslaGong @PPathole @priscillabanana https://...  1257550522132787200   \n",
      "7                      @PPathole @priscillabanana Boy  1257511340127555584   \n",
      "8                      @priscillabanana X √Ü A-12 Musk  1257508900812713984   \n",
      "9             @Gaelic_Neilson Mom &amp; baby all good  1257466633670610944   \n",
      "10  RT @SpaceX: #MayTheFourthBeWithYou https://t.c...  1257412412505862146   \n",
      "11                  @Gaelic_Neilson A few hours away!  1257401142813868032   \n",
      "12  @cleantechnica Anyway, we‚Äôre just talking abou...  1257400536179097601   \n",
      "13  @cleantechnica Weird that EPA would deny this....  1257398285716856834   \n",
      "14  @Erdayastronaut @SciGuySpace I hope anything l...  1257207162222215169   \n",
      "15  @NASASpaceflight @TesLatino @Erdayastronaut It...  1257204578669363200   \n",
      "16  @TesLatino @Erdayastronaut @NASASpaceflight Mo...  1257199336011649025   \n",
      "17                           @NASASpaceflight Thanks!  1257182656275480577   \n",
      "18  @NASASpaceflight Commentary is quite entertain...  1257182201130758146   \n",
      "19  SN4 üî• soon. Raptor looks so sm≈çl. https://t.co...  1257152194731622400   \n",
      "\n",
      "    len                date  likes  sentiment  \n",
      "0    52 2020-05-05 19:42:04     37         -1  \n",
      "1   140 2020-05-05 19:36:49    258          1  \n",
      "2    40 2020-05-05 19:32:36    808          0  \n",
      "3    70 2020-05-05 06:34:16  82972          0  \n",
      "4    72 2020-05-05 06:15:16   2377          1  \n",
      "5    88 2020-05-05 06:06:19   4498          1  \n",
      "6    61 2020-05-05 05:59:43  73393          0  \n",
      "7    30 2020-05-05 03:24:01  18118          0  \n",
      "8    30 2020-05-05 03:14:19  70347          0  \n",
      "9    39 2020-05-05 00:26:22  47602          2  \n",
      "10   58 2020-05-04 20:50:55      0          0  \n",
      "11   33 2020-05-04 20:06:08  14571         -1  \n",
      "12  137 2020-05-04 20:03:43   2788         -1  \n",
      "13  120 2020-05-04 19:54:47  13775          1  \n",
      "14   50 2020-05-04 07:15:19   3980          0  \n",
      "15   71 2020-05-04 07:05:03   1499         -1  \n",
      "16  140 2020-05-04 06:44:13   1266          1  \n",
      "17   24 2020-05-04 05:37:57   1673          1  \n",
      "18   56 2020-05-04 05:36:08   3725          1  \n",
      "19   57 2020-05-04 03:36:54  66864          0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['2020-05-05T19:42:04.000000000'],\n",
       "       ['2020-05-05T19:36:49.000000000'],\n",
       "       ['2020-05-05T19:32:36.000000000'],\n",
       "       ['2020-05-05T06:34:16.000000000'],\n",
       "       ['2020-05-05T06:15:16.000000000'],\n",
       "       ['2020-05-05T06:06:19.000000000'],\n",
       "       ['2020-05-05T05:59:43.000000000'],\n",
       "       ['2020-05-05T03:24:01.000000000'],\n",
       "       ['2020-05-05T03:14:19.000000000'],\n",
       "       ['2020-05-05T00:26:22.000000000'],\n",
       "       ['2020-05-04T20:50:55.000000000'],\n",
       "       ['2020-05-04T20:06:08.000000000'],\n",
       "       ['2020-05-04T20:03:43.000000000'],\n",
       "       ['2020-05-04T19:54:47.000000000'],\n",
       "       ['2020-05-04T07:15:19.000000000'],\n",
       "       ['2020-05-04T07:05:03.000000000'],\n",
       "       ['2020-05-04T06:44:13.000000000'],\n",
       "       ['2020-05-04T05:37:57.000000000'],\n",
       "       ['2020-05-04T05:36:08.000000000'],\n",
       "       ['2020-05-04T03:36:54.000000000'],\n",
       "       ['2020-05-04T02:57:58.000000000'],\n",
       "       ['2020-05-04T02:41:57.000000000'],\n",
       "       ['2020-05-04T02:40:53.000000000'],\n",
       "       ['2020-05-04T02:34:59.000000000'],\n",
       "       ['2020-05-04T02:31:41.000000000'],\n",
       "       ['2020-05-04T02:11:25.000000000'],\n",
       "       ['2020-05-04T02:10:38.000000000'],\n",
       "       ['2020-05-04T02:08:33.000000000'],\n",
       "       ['2020-05-04T01:37:19.000000000'],\n",
       "       ['2020-05-03T19:48:26.000000000'],\n",
       "       ['2020-05-03T19:10:40.000000000'],\n",
       "       ['2020-05-03T17:58:07.000000000'],\n",
       "       ['2020-05-03T08:08:44.000000000'],\n",
       "       ['2020-05-03T08:07:23.000000000'],\n",
       "       ['2020-05-03T08:03:50.000000000'],\n",
       "       ['2020-05-03T07:56:49.000000000'],\n",
       "       ['2020-05-03T07:48:03.000000000'],\n",
       "       ['2020-05-03T07:43:14.000000000'],\n",
       "       ['2020-05-03T07:16:25.000000000'],\n",
       "       ['2020-05-02T20:03:53.000000000'],\n",
       "       ['2020-05-02T19:55:34.000000000'],\n",
       "       ['2020-05-02T19:48:38.000000000'],\n",
       "       ['2020-05-02T19:48:09.000000000'],\n",
       "       ['2020-05-02T19:44:39.000000000'],\n",
       "       ['2020-05-02T19:43:20.000000000'],\n",
       "       ['2020-05-02T19:39:16.000000000'],\n",
       "       ['2020-05-02T11:14:50.000000000'],\n",
       "       ['2020-05-02T08:32:48.000000000'],\n",
       "       ['2020-05-02T08:30:10.000000000'],\n",
       "       ['2020-05-02T08:23:44.000000000'],\n",
       "       ['2020-05-02T08:03:18.000000000'],\n",
       "       ['2020-05-02T08:00:08.000000000'],\n",
       "       ['2020-05-02T07:33:42.000000000'],\n",
       "       ['2020-05-01T22:46:42.000000000'],\n",
       "       ['2020-05-01T22:44:56.000000000'],\n",
       "       ['2020-05-01T22:39:28.000000000'],\n",
       "       ['2020-05-01T22:34:24.000000000'],\n",
       "       ['2020-05-01T22:32:29.000000000'],\n",
       "       ['2020-05-01T22:20:07.000000000'],\n",
       "       ['2020-05-01T22:16:11.000000000'],\n",
       "       ['2020-05-01T21:46:14.000000000'],\n",
       "       ['2020-05-01T21:24:26.000000000'],\n",
       "       ['2020-05-01T16:24:20.000000000'],\n",
       "       ['2020-05-01T16:22:14.000000000'],\n",
       "       ['2020-05-01T16:17:43.000000000'],\n",
       "       ['2020-05-01T16:14:14.000000000'],\n",
       "       ['2020-05-01T16:08:57.000000000'],\n",
       "       ['2020-05-01T16:03:30.000000000'],\n",
       "       ['2020-05-01T15:30:38.000000000'],\n",
       "       ['2020-05-01T15:29:47.000000000'],\n",
       "       ['2020-05-01T15:28:08.000000000'],\n",
       "       ['2020-05-01T15:26:46.000000000'],\n",
       "       ['2020-05-01T15:15:13.000000000'],\n",
       "       ['2020-05-01T15:11:26.000000000'],\n",
       "       ['2020-05-01T15:10:24.000000000'],\n",
       "       ['2020-05-01T14:32:36.000000000'],\n",
       "       ['2020-05-01T14:19:34.000000000'],\n",
       "       ['2020-05-01T14:18:00.000000000'],\n",
       "       ['2020-05-01T14:14:14.000000000'],\n",
       "       ['2020-05-01T05:04:40.000000000'],\n",
       "       ['2020-05-01T03:09:15.000000000'],\n",
       "       ['2020-05-01T01:46:00.000000000'],\n",
       "       ['2020-04-30T21:20:17.000000000'],\n",
       "       ['2020-04-30T20:36:24.000000000'],\n",
       "       ['2020-04-30T18:32:27.000000000'],\n",
       "       ['2020-04-30T18:25:56.000000000'],\n",
       "       ['2020-04-30T17:54:59.000000000'],\n",
       "       ['2020-04-30T17:16:41.000000000'],\n",
       "       ['2020-04-30T17:16:03.000000000'],\n",
       "       ['2020-04-30T17:11:14.000000000'],\n",
       "       ['2020-04-30T17:11:10.000000000'],\n",
       "       ['2020-04-30T16:57:57.000000000'],\n",
       "       ['2020-04-30T14:56:31.000000000'],\n",
       "       ['2020-04-30T14:49:59.000000000'],\n",
       "       ['2020-04-30T14:49:01.000000000'],\n",
       "       ['2020-04-30T14:41:48.000000000'],\n",
       "       ['2020-04-30T14:41:18.000000000'],\n",
       "       ['2020-04-30T14:39:36.000000000'],\n",
       "       ['2020-04-30T14:35:05.000000000'],\n",
       "       ['2020-04-30T02:58:14.000000000'],\n",
       "       ['2020-04-30T02:08:48.000000000'],\n",
       "       ['2020-04-30T02:05:19.000000000'],\n",
       "       ['2020-04-30T02:02:52.000000000'],\n",
       "       ['2020-04-30T01:03:00.000000000'],\n",
       "       ['2020-04-30T00:51:32.000000000'],\n",
       "       ['2020-04-30T00:11:11.000000000'],\n",
       "       ['2020-04-29T23:50:58.000000000'],\n",
       "       ['2020-04-29T23:47:38.000000000'],\n",
       "       ['2020-04-29T23:46:44.000000000'],\n",
       "       ['2020-04-29T07:26:14.000000000'],\n",
       "       ['2020-04-29T06:42:14.000000000'],\n",
       "       ['2020-04-29T06:14:53.000000000'],\n",
       "       ['2020-04-29T06:08:29.000000000'],\n",
       "       ['2020-04-29T06:03:52.000000000'],\n",
       "       ['2020-04-29T01:02:09.000000000'],\n",
       "       ['2020-04-28T00:59:04.000000000'],\n",
       "       ['2020-04-27T23:58:23.000000000'],\n",
       "       ['2020-04-27T23:54:39.000000000'],\n",
       "       ['2020-04-27T23:54:13.000000000'],\n",
       "       ['2020-04-27T22:30:22.000000000'],\n",
       "       ['2020-04-27T22:17:44.000000000'],\n",
       "       ['2020-04-27T16:13:47.000000000'],\n",
       "       ['2020-04-27T16:13:21.000000000'],\n",
       "       ['2020-04-27T16:11:25.000000000'],\n",
       "       ['2020-04-27T16:09:24.000000000'],\n",
       "       ['2020-04-27T05:39:30.000000000'],\n",
       "       ['2020-04-27T05:18:51.000000000'],\n",
       "       ['2020-04-27T05:17:46.000000000'],\n",
       "       ['2020-04-27T05:15:03.000000000'],\n",
       "       ['2020-04-27T05:12:28.000000000'],\n",
       "       ['2020-04-27T04:45:49.000000000'],\n",
       "       ['2020-04-27T04:44:34.000000000'],\n",
       "       ['2020-04-27T04:28:04.000000000'],\n",
       "       ['2020-04-27T04:01:31.000000000'],\n",
       "       ['2020-04-27T03:50:33.000000000'],\n",
       "       ['2020-04-27T03:50:14.000000000'],\n",
       "       ['2020-04-27T03:00:41.000000000'],\n",
       "       ['2020-04-27T01:53:37.000000000'],\n",
       "       ['2020-04-27T01:40:05.000000000'],\n",
       "       ['2020-04-27T01:26:27.000000000'],\n",
       "       ['2020-04-27T00:31:20.000000000'],\n",
       "       ['2020-04-26T23:16:42.000000000'],\n",
       "       ['2020-04-26T22:54:12.000000000'],\n",
       "       ['2020-04-26T22:42:05.000000000'],\n",
       "       ['2020-04-26T19:42:51.000000000'],\n",
       "       ['2020-04-26T19:38:22.000000000'],\n",
       "       ['2020-04-26T16:16:28.000000000'],\n",
       "       ['2020-04-26T16:13:28.000000000'],\n",
       "       ['2020-04-26T16:13:17.000000000'],\n",
       "       ['2020-04-26T16:07:21.000000000'],\n",
       "       ['2020-04-26T15:36:07.000000000'],\n",
       "       ['2020-04-26T15:29:51.000000000'],\n",
       "       ['2020-04-26T15:27:11.000000000'],\n",
       "       ['2020-04-26T15:21:04.000000000'],\n",
       "       ['2020-04-25T13:33:39.000000000'],\n",
       "       ['2020-04-25T13:29:52.000000000'],\n",
       "       ['2020-04-25T13:17:51.000000000'],\n",
       "       ['2020-04-25T12:17:54.000000000'],\n",
       "       ['2020-04-25T12:16:57.000000000'],\n",
       "       ['2020-04-25T12:01:53.000000000'],\n",
       "       ['2020-04-25T11:58:12.000000000'],\n",
       "       ['2020-04-25T11:50:25.000000000'],\n",
       "       ['2020-04-25T11:48:01.000000000'],\n",
       "       ['2020-04-25T11:42:43.000000000'],\n",
       "       ['2020-04-25T10:50:09.000000000'],\n",
       "       ['2020-04-25T10:31:36.000000000'],\n",
       "       ['2020-04-25T10:18:39.000000000'],\n",
       "       ['2020-04-25T10:10:46.000000000'],\n",
       "       ['2020-04-25T04:26:16.000000000'],\n",
       "       ['2020-04-25T04:24:46.000000000'],\n",
       "       ['2020-04-25T04:20:51.000000000'],\n",
       "       ['2020-04-25T00:46:31.000000000'],\n",
       "       ['2020-04-25T00:08:32.000000000'],\n",
       "       ['2020-04-25T00:06:33.000000000'],\n",
       "       ['2020-04-25T00:03:55.000000000'],\n",
       "       ['2020-04-25T00:01:50.000000000'],\n",
       "       ['2020-04-24T23:56:01.000000000'],\n",
       "       ['2020-04-24T23:55:15.000000000'],\n",
       "       ['2020-04-24T23:52:50.000000000'],\n",
       "       ['2020-04-24T23:47:34.000000000'],\n",
       "       ['2020-04-24T22:28:54.000000000'],\n",
       "       ['2020-04-24T21:26:30.000000000'],\n",
       "       ['2020-04-24T21:18:22.000000000'],\n",
       "       ['2020-04-24T21:17:11.000000000'],\n",
       "       ['2020-04-24T18:51:14.000000000'],\n",
       "       ['2020-04-24T18:10:53.000000000'],\n",
       "       ['2020-04-24T04:59:19.000000000'],\n",
       "       ['2020-04-24T04:51:41.000000000'],\n",
       "       ['2020-04-24T04:32:12.000000000'],\n",
       "       ['2020-04-24T04:30:47.000000000'],\n",
       "       ['2020-04-24T00:30:31.000000000'],\n",
       "       ['2020-04-24T00:16:22.000000000'],\n",
       "       ['2020-04-24T00:13:06.000000000'],\n",
       "       ['2020-04-23T18:58:59.000000000'],\n",
       "       ['2020-04-23T18:44:33.000000000'],\n",
       "       ['2020-04-23T18:39:49.000000000'],\n",
       "       ['2020-04-23T17:57:02.000000000'],\n",
       "       ['2020-04-23T17:53:26.000000000'],\n",
       "       ['2020-04-23T17:49:16.000000000'],\n",
       "       ['2020-04-23T17:22:20.000000000']], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TweetAnalyzer():\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "#sentiment analysis based of polarity\n",
    "\n",
    "    def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        \n",
    "        if analysis.sentiment.polarity > 0.5:\n",
    "            return 2\n",
    "        elif analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity < 0:\n",
    "            return -1\n",
    "        elif analysis.sentiment.polarity < -0.5:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])\n",
    "\n",
    "        df['id'] = np.array([tweet.id for tweet in tweets])\n",
    "        df['len'] = np.array([len(tweet.text) for tweet in tweets])\n",
    "        df['date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "        df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "       \n",
    "        return df\n",
    "\n",
    " \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    twitter_client = TwitterClient()\n",
    "    tweet_analyzer = TweetAnalyzer()\n",
    "\n",
    "    api = twitter_client.get_twitter_client_api()\n",
    "\n",
    "    tweets = api.user_timeline(screen_name=\"elonmusk\", count=400)\n",
    "\n",
    "    df = tweet_analyzer.tweets_to_data_frame(tweets)\n",
    "    df['sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])\n",
    "\n",
    "    print(df.head(20))\n",
    "    \n",
    "    \n",
    "#create a new data frame with closing price \n",
    "sentiment = df.filter(['sentiment'])\n",
    "\n",
    "dates = df.filter(['date'])\n",
    "dates_set = dates.values\n",
    "#convert data frame to a Numpy array \n",
    "sentiment_set = sentiment.values\n",
    "\n",
    "dates_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[524.  ],\n",
       "       [481.56],\n",
       "       [454.47],\n",
       "       [480.01],\n",
       "       [516.24],\n",
       "       [545.45],\n",
       "       [548.84],\n",
       "       [573.  ],\n",
       "       [650.95],\n",
       "       [709.89],\n",
       "       [729.83],\n",
       "       [745.21],\n",
       "       [753.89],\n",
       "       [746.36],\n",
       "       [686.72],\n",
       "       [732.11],\n",
       "       [705.63],\n",
       "       [725.15],\n",
       "       [798.75],\n",
       "       [769.12],\n",
       "       [800.51]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "#define the ticker symbol\n",
    "tickerSymbol = 'tsla'\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "\n",
    "#get the historical prices for this ticker\n",
    "tickerDf = tickerData.history(period='1d', start='2020-4-1', end='2020-4-30')\n",
    "\n",
    "tickerDf\n",
    "\n",
    "#create a new data frame with closing price \n",
    "closing_price = tickerDf.filter(['Close'])\n",
    "#convert data frame to a Numpy array \n",
    "closing_price_set = closing_price.values\n",
    "\n",
    "closing_price_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given the array of prices, create an list of price changes. \n",
    "# An increase in stock price, compare to the previous day, will be represents with 1, \n",
    "# While a decrease in price will be represent with -1. \n",
    "\n",
    "\n",
    "price_change= [0] * (len(closing_price_set)-1)\n",
    "for i in range(0, (len(closing_price_set)-1)):\n",
    "    if closing_price_set[i+1] > closing_price_set[i]:\n",
    "        price_change[i]= 1\n",
    "    else:\n",
    "        price_change[i]= -1\n",
    "        \n",
    "price_change\n",
    "\n",
    "# Create an array with daily sentiment.\n",
    "# Daily setiment is defined as the sum of all the tweets' sentiment score in a given day\n",
    "\n",
    "\n",
    "daily_sentiment= [0] *(len(closing_price_set)-1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
